# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил(а):
- Ямбушев Артём Дамирович
- РИ-210933
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | # | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Выводы.
- ✨Magic ✨

## Цель работы
### Интеграция экономической системы в проект Unity и обучение ML-Agent

## Задание 1
### Измените параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели.

- Для начала я открыл проект Unity готовый к обучению. Внимательно изучил: каждый объект на сцене (их хар-ки и свойства); скрипт-файл Move, позволяющий объектам взаимодействовать друг с другом; остальные файлы находящиеся в папке проекта; а особенно Economic.yaml, который является конфигуратором обучающего тренажера, так как именно он и является yaml-агентом с которым мне нужно было провести работу.
- Для ускорения обучения увеличил количество префабов до 12.
- Как выглядит проект Unity:

![image](https://user-images.githubusercontent.com/101344196/205086529-d3ff2f02-0ac3-4fae-9f06-7de3cbf8d1dc.png)

- Далее запустил обучение MLAgent через Anaconda Prompt для проверки работоспособности проекта. Вывод консоли был правильным:

![image](https://user-images.githubusercontent.com/101344196/205090409-146f5e45-cdda-409a-bedb-bc20a0d76a5b.png)

- Следующим шагом я установил TensorBoard, чтобы была возможность построения графиков для оценки результатов обучения.
- Запустил TensorBoard и открыл вкладку с графиками в браузере: 

![image](https://user-images.githubusercontent.com/101344196/205093958-49914269-15c3-420c-bcba-dc98408f3d26.png)

![image](https://user-images.githubusercontent.com/101344196/205107278-7a237c1b-a4ff-4aa2-9bc2-5decf7da84a3.png)

- **С этого момента начал экспериментировать с параметрами yaml-файла, паралельно переобучая RollerAgent и создавая новые графики**
- Сам yaml-файл:

```yaml
behaviors:
  Economic:
    trainer_type: ppo
    hyperparameters:
      batch_size: 1024
      buffer_size: 10240
      learning_rate: 3.0e-4
      learning_rate_schedule: linear
      beta: 1.0e-2
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3      
    network_settings:
      normalize: false
      hidden_units: 128
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    checkpoint_interval: 500000
    max_steps: 750000
    time_horizon: 64
    summary_freq: 5000
    self_play:
      save_steps: 20000
      team_change: 100000
      swap_steps: 10000
      play_against_latest_model_ratio: 0.5
      window: 10
```

- **batch_size** уменьшил значение. Определяет то, сколько нужно провести опытов в каждой итерации **нахождения локального минимума или максимума функции с помощью движения вдоль градиента (Градиентного спуска)**.

![image](https://user-images.githubusercontent.com/101344196/205138575-f65a8f1c-446c-43a0-8d43-5aae2f641036.png)

- **buffer_size** уменьшил в 10 раз. Определяет количество опыта, которое должно быть собрано, прежде чем мы будем изучать или обновлять модель.

![image](https://user-images.githubusercontent.com/101344196/205109430-5668965a-4a11-4d90-b527-97ba38bbf73f.png)

- **learning_rate** увеличил во много раз. Определяет силу каждого шага обновления градиентного спуска.

![image](https://user-images.githubusercontent.com/101344196/205111554-4a5fbbff-bc83-4ff3-8b64-38f3b31b6fb9.png)

- **learning_rate_schedule** Определяет, как скорость обучения изменяется с течением времени. Значение linear делает так чтобы, скорость обучения уменьшалась линейно, достигая 0 на max_steps.

- **beta** увеличил в 4 раза. Гарантирует, что агенты должным образом исследуют пространство действия во время обучения. Увеличение этого параметра обеспечит выполнение большего количества случайных действий. 

![image](https://user-images.githubusercontent.com/101344196/205113340-d1557cfc-fd8f-45f0-8f3c-922cc6f8fd0e.png)

- **epsilon** вначале увеличил в 4 раза, но из-за большого количества операций приложение Unity перестало работать, поэтому изменил увеличение только в 2 раза. Влияет на скорость изменения подбора новых значений во время обучения. Так как я его значительно увеличил, RollerAgent с первой же итерации смог успешно справиться с задачей.

![image](https://user-images.githubusercontent.com/101344196/205114895-78d383ee-7953-49de-a030-96a95b65b166.png)

- **lambd** уменьшил. Определяет то, насколько агент полагается на свою текущую оценку успешности при вычислении обновленных значений успешности.

![image](https://user-images.githubusercontent.com/101344196/205115726-4de79499-28e7-472b-9f10-8897998d5ae7.png)

- **num_epoch** увеличил. Определяет количество проходов через буфер опыта при выполнении оптимизации градиентного спуска.

![image](https://user-images.githubusercontent.com/101344196/205117011-e6aca9ce-e2dc-47ab-b2f2-b4c199e1bf7f.png)

- **normalize** поставил true. Определяет применяется ли нормализация к входным данным векторных наблюдений.

- **hidden_units** Определяет количество единиц в каждом полносвязном слое нейронной сети. По сути не сильно влияет на обучение.
- **num_layers** Определяет количество скрытых слоев в нейронной сети. Мало влияет на обучение.
- **gamma** Этот параметр можно рассматривать как то, как далеко в будущем агент должен заботиться о возможных вознаграждениях.
- **strength** Коэффициент, на который умножается вознаграждение, данное средой.

- **checkpoint_interval** уменьшил в два раза. Определяет то, сколько нужно получить опыта чтобы перейти контрольную точку, для сохранения onnx файла, например. Не влияет на обучение.

![image](https://user-images.githubusercontent.com/101344196/205118487-4136f6f6-fd10-47af-9798-2a8439aa29d7.png)

- **max_steps** прибавил 25 тысяч. Определяет количество шагов, которые необходимо выполнить в среде перед завершением процесса обучения. Влияет на конечный результат обучения, которого придётся очень долго дожидаться.

![image](https://user-images.githubusercontent.com/101344196/205119985-65f301ef-ad0f-408e-9b6d-56cccf43545e.png)

- **time_horizon** Определяет сколько шагов опыта необходимо собрать для каждого агента, прежде чем добавить его в буфер опыта.
- **summary_freq** Определяет количество очков опыта, которые необходимо собрать перед созданием и отображением статистики обучения. Это определяет детализацию графиков в Tensorboard.
- **save_steps** Определяет количество шагов тренера между снимками.
- **team_change** Определяет количество шагов тренера между переключением обучающей команды.
- **swap_steps** Количество шагов пустышек (не шагов тренера) между заменой политики оппонентов на другой снимок.
- **play_against_latest_model_ratio** Вероятность того, что агент будет играть против последнего результата оппонента.
- **window** Определяет максимальное количество сохранённых прошлых снимков, из которых выбираются противники агента.

## Задание 2

## Выводы
Я, выполняя эту лабораторную работу, познакомился с TensorBoard и параметрами из стандартных yaml-файлов. Научился определять влияние параметров на обучение MLAgent. Интегрировал экономическую систему в проект Unity. Связал обучение MLAgent для экономической системы. 
